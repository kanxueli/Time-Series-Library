########## Parameters number for all 40.13 M  #########
Parameter: enc_pos_embedding, Shape: torch.Size([1, 4, 3, 512])
Parameter: dec_pos_embedding, Shape: torch.Size([1, 4, 1, 512])
Parameter: enc_value_embedding.value_embedding.weight, Shape: torch.Size([512, 12])
Parameter: pre_norm.weight, Shape: torch.Size([512])
Parameter: pre_norm.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.router, Shape: torch.Size([3, 3, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.time_attention.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.time_attention.query_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.time_attention.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.time_attention.key_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.time_attention.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.time_attention.value_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.time_attention.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.time_attention.out_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_sender.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_sender.query_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_sender.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_sender.key_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_sender.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_sender.value_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_sender.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_sender.out_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_receiver.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_receiver.query_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_receiver.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_receiver.key_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_receiver.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_receiver.value_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_receiver.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.dim_receiver.out_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.norm1.weight, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.norm1.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.norm2.weight, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.norm2.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.norm3.weight, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.norm3.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.norm4.weight, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.norm4.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.MLP1.0.weight, Shape: torch.Size([2048, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.MLP1.0.bias, Shape: torch.Size([2048])
Parameter: encoder.encode_blocks.0.encode_layers.0.MLP1.2.weight, Shape: torch.Size([512, 2048])
Parameter: encoder.encode_blocks.0.encode_layers.0.MLP1.2.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.0.encode_layers.0.MLP2.0.weight, Shape: torch.Size([2048, 512])
Parameter: encoder.encode_blocks.0.encode_layers.0.MLP2.0.bias, Shape: torch.Size([2048])
Parameter: encoder.encode_blocks.0.encode_layers.0.MLP2.2.weight, Shape: torch.Size([512, 2048])
Parameter: encoder.encode_blocks.0.encode_layers.0.MLP2.2.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.merge_layer.linear_trans.weight, Shape: torch.Size([512, 1024])
Parameter: encoder.encode_blocks.1.merge_layer.linear_trans.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.merge_layer.norm.weight, Shape: torch.Size([1024])
Parameter: encoder.encode_blocks.1.merge_layer.norm.bias, Shape: torch.Size([1024])
Parameter: encoder.encode_blocks.1.encode_layers.0.router, Shape: torch.Size([2, 3, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.time_attention.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.time_attention.query_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.time_attention.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.time_attention.key_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.time_attention.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.time_attention.value_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.time_attention.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.time_attention.out_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_sender.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_sender.query_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_sender.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_sender.key_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_sender.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_sender.value_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_sender.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_sender.out_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_receiver.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_receiver.query_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_receiver.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_receiver.key_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_receiver.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_receiver.value_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_receiver.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.dim_receiver.out_projection.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.norm1.weight, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.norm1.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.norm2.weight, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.norm2.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.norm3.weight, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.norm3.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.norm4.weight, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.norm4.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.MLP1.0.weight, Shape: torch.Size([2048, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.MLP1.0.bias, Shape: torch.Size([2048])
Parameter: encoder.encode_blocks.1.encode_layers.0.MLP1.2.weight, Shape: torch.Size([512, 2048])
Parameter: encoder.encode_blocks.1.encode_layers.0.MLP1.2.bias, Shape: torch.Size([512])
Parameter: encoder.encode_blocks.1.encode_layers.0.MLP2.0.weight, Shape: torch.Size([2048, 512])
Parameter: encoder.encode_blocks.1.encode_layers.0.MLP2.0.bias, Shape: torch.Size([2048])
Parameter: encoder.encode_blocks.1.encode_layers.0.MLP2.2.weight, Shape: torch.Size([512, 2048])
Parameter: encoder.encode_blocks.1.encode_layers.0.MLP2.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.router, Shape: torch.Size([1, 3, 512])
Parameter: decoder.decode_layers.0.self_attention.time_attention.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.time_attention.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.time_attention.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.time_attention.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.time_attention.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.time_attention.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.time_attention.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.time_attention.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.dim_sender.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.dim_sender.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.dim_sender.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.dim_sender.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.dim_sender.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.dim_sender.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.dim_sender.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.dim_sender.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.dim_receiver.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.dim_receiver.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.dim_receiver.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.dim_receiver.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.dim_receiver.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.dim_receiver.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.dim_receiver.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.self_attention.dim_receiver.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.norm1.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.norm1.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.norm2.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.norm2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.norm3.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.norm3.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.norm4.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.norm4.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.MLP1.0.weight, Shape: torch.Size([2048, 512])
Parameter: decoder.decode_layers.0.self_attention.MLP1.0.bias, Shape: torch.Size([2048])
Parameter: decoder.decode_layers.0.self_attention.MLP1.2.weight, Shape: torch.Size([512, 2048])
Parameter: decoder.decode_layers.0.self_attention.MLP1.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.self_attention.MLP2.0.weight, Shape: torch.Size([2048, 512])
Parameter: decoder.decode_layers.0.self_attention.MLP2.0.bias, Shape: torch.Size([2048])
Parameter: decoder.decode_layers.0.self_attention.MLP2.2.weight, Shape: torch.Size([512, 2048])
Parameter: decoder.decode_layers.0.self_attention.MLP2.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.cross_attention.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.cross_attention.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.cross_attention.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.cross_attention.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.cross_attention.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.cross_attention.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.cross_attention.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.cross_attention.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.norm1.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.norm1.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.norm2.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.norm2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.MLP1.0.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.MLP1.0.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.MLP1.2.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.0.MLP1.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.0.linear_pred.weight, Shape: torch.Size([12, 512])
Parameter: decoder.decode_layers.0.linear_pred.bias, Shape: torch.Size([12])
Parameter: decoder.decode_layers.1.self_attention.router, Shape: torch.Size([1, 3, 512])
Parameter: decoder.decode_layers.1.self_attention.time_attention.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.time_attention.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.time_attention.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.time_attention.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.time_attention.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.time_attention.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.time_attention.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.time_attention.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.dim_sender.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.dim_sender.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.dim_sender.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.dim_sender.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.dim_sender.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.dim_sender.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.dim_sender.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.dim_sender.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.dim_receiver.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.dim_receiver.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.dim_receiver.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.dim_receiver.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.dim_receiver.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.dim_receiver.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.dim_receiver.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.self_attention.dim_receiver.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.norm1.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.norm1.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.norm2.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.norm2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.norm3.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.norm3.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.norm4.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.norm4.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.MLP1.0.weight, Shape: torch.Size([2048, 512])
Parameter: decoder.decode_layers.1.self_attention.MLP1.0.bias, Shape: torch.Size([2048])
Parameter: decoder.decode_layers.1.self_attention.MLP1.2.weight, Shape: torch.Size([512, 2048])
Parameter: decoder.decode_layers.1.self_attention.MLP1.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.self_attention.MLP2.0.weight, Shape: torch.Size([2048, 512])
Parameter: decoder.decode_layers.1.self_attention.MLP2.0.bias, Shape: torch.Size([2048])
Parameter: decoder.decode_layers.1.self_attention.MLP2.2.weight, Shape: torch.Size([512, 2048])
Parameter: decoder.decode_layers.1.self_attention.MLP2.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.cross_attention.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.cross_attention.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.cross_attention.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.cross_attention.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.cross_attention.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.cross_attention.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.cross_attention.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.cross_attention.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.norm1.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.norm1.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.norm2.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.norm2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.MLP1.0.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.MLP1.0.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.MLP1.2.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.1.MLP1.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.1.linear_pred.weight, Shape: torch.Size([12, 512])
Parameter: decoder.decode_layers.1.linear_pred.bias, Shape: torch.Size([12])
Parameter: decoder.decode_layers.2.self_attention.router, Shape: torch.Size([1, 3, 512])
Parameter: decoder.decode_layers.2.self_attention.time_attention.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.time_attention.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.time_attention.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.time_attention.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.time_attention.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.time_attention.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.time_attention.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.time_attention.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.dim_sender.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.dim_sender.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.dim_sender.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.dim_sender.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.dim_sender.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.dim_sender.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.dim_sender.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.dim_sender.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.dim_receiver.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.dim_receiver.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.dim_receiver.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.dim_receiver.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.dim_receiver.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.dim_receiver.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.dim_receiver.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.self_attention.dim_receiver.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.norm1.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.norm1.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.norm2.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.norm2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.norm3.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.norm3.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.norm4.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.norm4.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.MLP1.0.weight, Shape: torch.Size([2048, 512])
Parameter: decoder.decode_layers.2.self_attention.MLP1.0.bias, Shape: torch.Size([2048])
Parameter: decoder.decode_layers.2.self_attention.MLP1.2.weight, Shape: torch.Size([512, 2048])
Parameter: decoder.decode_layers.2.self_attention.MLP1.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.self_attention.MLP2.0.weight, Shape: torch.Size([2048, 512])
Parameter: decoder.decode_layers.2.self_attention.MLP2.0.bias, Shape: torch.Size([2048])
Parameter: decoder.decode_layers.2.self_attention.MLP2.2.weight, Shape: torch.Size([512, 2048])
Parameter: decoder.decode_layers.2.self_attention.MLP2.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.cross_attention.query_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.cross_attention.query_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.cross_attention.key_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.cross_attention.key_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.cross_attention.value_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.cross_attention.value_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.cross_attention.out_projection.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.cross_attention.out_projection.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.norm1.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.norm1.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.norm2.weight, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.norm2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.MLP1.0.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.MLP1.0.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.MLP1.2.weight, Shape: torch.Size([512, 512])
Parameter: decoder.decode_layers.2.MLP1.2.bias, Shape: torch.Size([512])
Parameter: decoder.decode_layers.2.linear_pred.weight, Shape: torch.Size([12, 512])
Parameter: decoder.decode_layers.2.linear_pred.bias, Shape: torch.Size([12])